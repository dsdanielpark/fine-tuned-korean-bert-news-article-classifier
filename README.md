# Korean-news-topic-classification-using-KO-BERT(public)

> ğŸš¸ **Be careful when cloning this repo**: It contains large NLP model weights. (>0.3GB, [`git-lfs`](https://git-lfs.com/))

<br> 

## TASK: Multi-category(8 classes) Korean News Topic Classifier í•œê¸€ ë‰´ìŠ¤ í† í”½ ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ <Br>
- Perform a simple task to compare the performance of KO-BERT and implementations of BERT in different frameworks <br>
- FrameWork: `PyTorch`, `APACHE MXNET GluonNLP` and `Hugging-Face` for modeling, `Scikit-Learn` to calculate evaluation metric.
- Pre-trained model: [`KO-BERT`](https://github.com/SKTBrain/KoBERT), `BERT`, [`SBERT`](https://www.sbert.net/)
- Dataset: Korean News Topic Classification Dataset (íŠ¹ì • íšŒì‚¬ì—ì„œ ìˆ˜ì§‘í•˜ì—¬ ë¹„ê³µê°œë¡œ ì œê³µ) <br>
- Evaluation: Accuracy <br>
- Duration: Jan 16,2023 - Jan 19,2023 (4days) <br>
- Author: [Daniel Park, South Korea](https://github.com/DSDanielPark) <br>

<br>

# TL ; DR
### Main Task: í•œêµ­ì–´ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ í† í”½(8ê°œ) ë¶„ë¥˜ ëª¨ë¸(BERT Classifier) ìƒì„±
### Sub Task 
- (í”„ë ˆì„ì›Œí¬ ë¹„êµ) `GluonNLP`, `PyTorch`, `Hugging-Face` í”„ë ˆì„ì›Œí¬ì˜ BERT ëª¨ë¸ êµ¬í˜„ ë¹„êµ
- (ì¸í’‹ ì •ë³´ëŸ‰ì— ë”°ë¥¸ BERT Classifierì˜ ì„±ëŠ¥ ë¹„êµ) ë‰´ìŠ¤ì˜ ì œëª©, ì œëª©+ë³¸ë¬¸, ë³¸ë¬¸ì„ ì¸í’‹ìœ¼ë¡œ ì‚¬ìš©í•˜ì˜€ì„ ê²½ìš°ì˜ ëª¨ë¸ ë¶„ë¥˜ ì„±ëŠ¥ ì •ëŸ‰ì  ë¹„êµ
- (ì‚¬ì „ êµ°ì§‘í™”ì •ë³´ì— ë”°ë¥¸ ëª¨ë¸ í•™ìŠµ ì†ë„ ë¹„êµ) Multilangual BERT(SBERT)ë¥¼ í†µí•´ ì‚¬ì „ì— êµ°ì§‘ ì •ë³´ë¥¼ ì£¼ì—ˆì„ ê²½ìš°, ëª¨ë¸ì˜ í•™ìŠµ ì†ë„ê°€ ë‹¬ë¼ì§€ëŠ”ì§€ ì—¬ë¶€ ë¹„êµ
  - PyTorchì˜ ê²½ìš°, initial [CLS] í† í°ì— ì •ë³´ë¥¼ ì„ë² ë”©í•˜ê³ 
  - GluonNLPì˜ ê²½ìš°, ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ êµ°ì§‘ì— ëŒ€í•œ ì •ë³´ë¥¼ ì¸í’‹ì— ì‚½ì…í•˜ì—¬ ë¹„êµ
### ë³¸ ë ˆí¬ì§€í† ë¦¬ëŠ”
- MXNet GluonNLP BERT weight íŒŒì¼ì„ git-lfsì„ í†µí•´ì„œ ì œê³µí•˜ê³ , `data/sample.csv`ë¥¼ ì œê³µí•©ë‹ˆë‹¤. 
- ì¶”í›„ ë²”ìš©ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íŒ¨í‚¤ì§€ pypië¥¼ í†µí•´ ë°°í¬í•©ë‹ˆë‹¤. 
- To-Do: Hugging Face í”„ë ˆì„ì›Œí¬, XAI, GPT2, GPT3, BERT Pipeline etc.


<br><br><br>

## Repository folder tree
```
ğŸ“¦fine-tuned-korean-BERT-news-article-classifier 
 â”£ ğŸ“‚data
 â”ƒ â”£ ğŸ“‚csv
 â”ƒ â”£ ğŸ“‚imgs
 â”ƒ â”£ ğŸ“œsample.csv                          # sample data will provide
 â”ƒ â”£ ğŸ“œtest_set.csv
 â”ƒ â”— ğŸ“œtrain_set.csv
 
 â”£ ğŸ“‚experiments                           # dummy for experiments
 â”ƒ â”£ ğŸ“‚experiment_weights
 â”ƒ â”£ ğŸ“œexp.md
 â”ƒ â”— ğŸ“œexp_metric.md
 
 â”£ ğŸ“‚notebooks                             # notebook will provide

 â”£ ğŸ“‚src
 â”ƒ â”£ ğŸ“‚kobert                              # SKT KOBERT
 â”ƒ â”£ ğŸ“‚kobert_gluon                        # gloun nlp í”„ë ˆì„ì›Œí¬ ì‹¤í—˜ì„ ìœ„í•´ ìƒì„±í•œ ëª¨ë“ˆ
 â”ƒ â”£ ğŸ“‚kobert_hf                           # SKT KOBERT
 â”ƒ â”£ ğŸ“‚kobert_pytorch                      # torch bert ì‹¤í—˜ì„ ìœ„í•´ ìƒì„±í•œ ëª¨ë“ˆ
 â”ƒ â”£ ğŸ“‚preprocess                          # ë³¸ ë ˆí¬ì§€í† ë¦¬ ì‹¤í—˜ì„ ìœ„í•œ ì „ì²˜ë¦¬ í´ë˜ìŠ¤

 â”£ ğŸ“‚weights
 â”ƒ â”£ ğŸ“œko-news-clf-gluon-weight.pth        # provide throught git-lfs (0.3 GB)
 â”ƒ â”— ğŸ“œko-news-clf-torch-weight.pth        # will not provide (>1.0 GB)

 â”£ ğŸ“œ.gitattributes                        # git-lfs managing
 â”£ ğŸ“œ.gitignore
 â”£ ğŸ“œconfig.py                             # config
 â”£ ğŸ“œLICENSE
 â”£ ğŸ“œmain.py                               # main.py (gluon inferenceë§Œ ì œê³µ)
 â”£ ğŸ“œREADME.md
 â”— ğŸ“œrequirements.txt
```




<br><Br>
## [Optional] Related Packages
#### 1. [`QuickShow`](https://pypi.org/project/quickshow/): pandas.DataFrameì„ ì¸í’‹ìœ¼ë¡œ ì‰½ê³  ë¹ ë¥´ê²Œ ì‹œê°í™”í•  ìˆ˜ ìˆëŠ” íŒ¨í‚¤ì§€
```bash
$ pip install quickshow
```
- ì´ë²ˆ í”„ë¡œì íŠ¸ì— í™œìš©ëœ ì¼ë¶€ ì‹œê°í™” ëª¨ë“ˆì„ ë°°í¬í•˜ì˜€ìŠµë‹ˆë‹¤. ì§€ê¸ˆê¹Œì§€ ì œê°€ ì—¬ëŸ¬ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©ëœ ë‹¤ì–‘í•œ ëª¨ë“ˆì„ í¸ë¦¬í•˜ê²Œ ë˜í•‘í•˜ì—¬ ì¶”í›„ ì—…ë°ì´íŠ¸í•  ì˜ˆì •ì…ë‹ˆë‹¤.


<br><br>

## Quick Start
ë³¸ ë ˆí¬ì§€í† ë¦¬ëŠ” highly encapsulationëœ gluon weight bert classifierì˜ inference classë¥¼ ì œê³µí•©ë‹ˆë‹¤.
```
$ git clone https://github.com/DSDanielPark/fine-tuned-korean-BERT-news-article-classifier.git
$ cd fine-tuned-korean-BERT-news-article-classifier
$ pip install -e .
$ python main.py
>>> Predicted news topic: international
```


<Br>

You use this optional args as below.
```
for gluon weight inference only

optional arguments:
  -h, --help            show this help message and exit
  --gluon_weight_path GLUON_WEIGHT_PATH                        # glouon weight file path
  --data_path DATA_PATH                                        # input csv data path
  --save_path SAVE_PATH                                        # save csv full path
```

<br>
<br>

## Remark
- *Please note that the domain was not fully searched because it is not mission critical artificial intelligence and is intended to simply identify NLP(Natural Language Processing) models in korean and perform requested tasks in a short period of time with limited data.*
- If it is used in actual service, some more tricks or experiments may be required. but as mentioned above, it is expected that the service can be maintained satisfactorily if applied in the same way with the heuristic method. There may be considerations such as continuous learning.
<!-- - *This is just a toy project! please enjoy it!* <br>
![](https://github.com/DSDanielPark/news-article-classification-using-koBERT/blob/main/imgs/enjoy2.gif) -->
<br>

## Outline
- **Problem Definition:** 
<br> Create a model that classifies articles into the following 8 categories with the title and body as input.
<br> `Categories` = ['society', 'politics', 'international', 'economy', 'sport', 'entertain', 'it', 'culture'] <br><br>
- **Data Description:**
<br> The Total 3,000 korean news dataset(Train: 2,100 articles, Test 900: articles, 7:3 split) consist of title, body and category columns. <br><br>


        ```
        title: korean article title
        cleanBody: korean article body
        category: of article (eight classes)
        ```



  |  | title |cleanBody|category|
  |---:|:---|:---|:---|
  |  0 | ë³´ê±´ë³µì§€ë¶€, ìƒˆí•´ ì²­ë…„ê³¼ ì§€ì—­ì´ í•¨ê»˜ ì§€ì—­ì‚¬íšŒì„œë¹„ìŠ¤ë¥¼ ê°œë°œì œê³µí•´ ì‚¬íšŒì„œë¹„ìŠ¤ ê³ ë„í™”ì— ì•ì¥ì„œ | ë³´ê±´ë³µì§€ë¶€(ì¥ê´€ ì¡°ê·œí™)ëŠ” 2023ë…„ ì²­ë…„ì‚¬íšŒì„œë¹„ìŠ¤ì‚¬ì—…ë‹¨(ì´í•˜ â€˜ì²­ë…„ì‚¬ì—…ë‹¨â€™)ì„ ì • ë° ìš´ì˜ì„ ìœ„í•´ ì˜¤ëŠ” 1ì›” 27ì¼(ê¸ˆ)ë¶€í„° 2ì›” 15ì¼(ìˆ˜)ê¹Œì§€ ì•½ 3ì£¼ê°„ ì „êµ­ 17ê°œ ì‹œë„(ê´‘ì—­ ì§€ìì²´)ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì²­ë…„ì‚¬ì—…ë‹¨ ê³µëª¨ë¥¼ ì‹¤ì‹œí•œë‹¤ê³  ë°í˜”ìŠµë‹ˆë‹¤. ...(ì¤‘ëµ)... ë³¸ ê¸°ì‚¬ëŠ” ê¹ƒí—ˆë¸Œ ë°ì´í„° ì˜ˆì‹œë¥¼ ìœ„í•´ ë³´ê±´ë³µì§€ë¶€ ë³´ë„ìë£Œë¥¼ ì°¸ì¡°í•˜ì—¬ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤. | society    |
  |  1 | BTS, 10ë…„ê°„ ë¹Œë³´ë“œ 'Hot 100' ì°¨íŠ¸ 1ìœ„ê³¡ ìµœë‹¤ ë³´ìœ  ê°€ìˆ˜ë¡œ ê¸°ë¡ë¼ | ë°©íƒ„ì†Œë…„ë‹¨(ì´í•˜ "BTS")ê°€ ë¯¸êµ­ ë¹Œë³´ë“œ ë©”ì¸ ì‹±ê¸€ ì°¨íŠ¸ 'HOT 100'ì˜ ì •ìƒì— ê°€ì¥ ë§ì´ ì˜¤ë¥¸ ì•„í‹°ìŠ¤íŠ¸ë¡œ ì„ ì •ë˜ì—ˆìŠµë‹ˆë‹¤. BTSê°€ ë¯¸êµ­ ìœ ëª… ê°€ìˆ˜ ë“œë ˆì´í¬ë‚˜ ì•„ë¦¬ì•„ë‚˜ ê·¸ë€ë° ë“±ì„ ì œì¹˜ê³ , ë©”ì¸ ì‹±ê¸€ ì°¨íŠ¸ í•« 100ì— ì˜¬ë¼ ë‹¤ì‹œ í•œë²ˆ K-POPì˜ ìœ„ìƒì„ ì„¸ìƒì— ë–¨ì³¤ìŠµë‹ˆë‹¤. ...(ì¤‘ëµ)... BTSëŠ” ìƒˆë¡œìš´ ìŒì•…ê³¼ ì•ˆë¬´ë¡œ ë‹¤ì‹œ ì»´ë°±ì„ ì˜ˆê³ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ìƒ ê¹ƒí—ˆë¸Œ ë‰´ìŠ¤ ë‹¤ë‹ˆì—˜ ê¸°ìì˜€ìŠµë‹ˆë‹¤. | entertain |


<br><Br>

- **EDA:**
<br> I analyzed the quality of the data, but I couldn't find any outliers because it was a very well-refined dataset. 
<br> Since this data was processed for a specific purpose by a Korean IT company, data preprocessing was unnecessary. 
<br> Replace the next image with data analysis.
<br><Br>
<img src="./data/imgs/fig.png" width="600"> <br> *Fig 1. í…ŒìŠ¤íŠ¸ì…‹ê³¼ íŠ¸ë ˆì¸ ì…‹ì˜ ë°ì´í„° ë¶„í¬(ì´ íƒœìŠ¤í¬ì˜ ê²½ìš°, ì¶”ê°€ì ì¸ ë°ì´í„° ìˆ˜ì§‘ ë° í•™ìŠµì´ ìš©ì´í•˜ë¯€ë¡œ imbalanceëŠ” ê³ ë ¤í•˜ì§€ ì•ŠìŒ)*
<br><br>
  - Since the advent of large-scale natural language processing models, feature engineering on corpus has not been of great significance.
  - In `MODE 4` and `MODE 5`, a test was conducted to give some information on the similarity of articles in advance through k-mean clustering based on the tokens embedded from the pre-trained BERT model.

  <br>

- **Input Features list:**
  1. *input_ids* - are the ids associated with each token as per BERT vocabulary.
  2. *input_mask* - differentiate between padding and real tokens.
  3. *Segment_ids* - will be a list of 0â€™s, as in classification task there is only a single text.
  4. *label_id* - corresponds to Label Encoder classes
<br><br>



# Results
All model architecture and learning conditions in the whole pipeline were fixed except the initial layer in MODE5. <br>
- Mode1 to Mode4: `MXNet` Framework
- Mode5: `Pytorch` Framework to modify the initial layer
- Pre-trained Models are KO-BERT [3], SBERT [5].

  | |`MODE 1`|`MODE 2`|`MODE 3`|`MODE 4`|`MODE 5`|
  |:---:|:---:|:---:|:---:|:---:|:---:|
  |Data|article body only|articel title only|article with title|article with title which have clustered information from SBERT model|article with title|
  |Model|KO-BERT|KO-BERT|KO-BERT|KO-BERT, SBERT|KO-BERT, SBERT with clustered information in initial hidden layer|
  |TestSet Accuracy|0.8895|0.8269|0.8864|0.8895|-|
  |Remark|Model architecture and all conditions in models were fixed.|-|-|-|Model architecture and initial layer are changed only in `MODE5`.|
- As I mentioned in the 'Experiments' section, All conditions in the whole pipeline were fixed except the initial layer in MODE5 and Pre-trained Models were KO-BERT [3], SBERT [5].



<br><br>


## Experiments


- ë³¸ ì‹¤í—˜ ì•„ì´ë””ì–´ë¥¼ ìœ„í•´ ì°¸ê³ í•œ ë…¼ë¬¸ë“¤ì€ ì—†ìœ¼ë‚˜, í›„í–¥ì  ë¶„ì„ê³¼ì •ì—ì„œ ë¹„ìŠ·í•œ ì»¨ì…‰ì˜ ë…¼ë¬¸ë“¤ì„ ë°œê²¬í•˜ì—¬ `References`ì— ê¸°ì¬í•¨.
- ë³¸ ì‹¤í—˜ë“¤ì€ í•œêµ­ì–´ë¥¼ í†µí•œ ì‹¤ì§ˆì ì¸ íƒœìŠ¤í¬ ì ìš©ì‹œì— ì°¸ê³ ìš©ìœ¼ë¡œ ìˆ˜í–‰.<br><br>

- ì‹¤í—˜ì€ í¬ê²Œ ë‹¤ìŒ 4ê°€ì§€ ê°€ì„¤ì„ í™•ì¸í•˜ê¸° ìœ„í•˜ì—¬ ì„¤ê³„ë˜ì—ˆìŒ. <br>
  1) í•œê¸€ ê¸°ì‚¬ì˜ ë³¸ë¬¸, ì œëª©, ë³¸ë¬¸ê³¼ ì œëª©ì„ í†µí•œ ë¶„ë¥˜ ì„±ëŠ¥ì˜ ì°¨ì´ í™•ì¸í•˜ê³ ì ì„¤ê³„ë¨ - `MODE1~MODE3`
  2) ì¼ì¢…ì˜ ì§€ì‹ì¦ë¥˜ë¥¼ í†µí•œ KO-BERTì˜ ë¯¸ì„¸ ì¡°ì •(fine-tunning) ê³¼ì •ì—ì„œ Teacher Model(SBERT)ë¡œ ë¶€í„° ìƒì„±í•œ êµ°ì§‘ì— ëŒ€í•œ ì •ë³´ê°€ ëª¨ë¸ ìˆ˜ë ´ì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ ì ì„¤ê³„ë¨ - `MODE3~MODE4`
  3) 2ë²ˆì˜ ê³¼ì •ì—ì„œ SBERTë¡œ ìƒì„±ëœ ì •ë³´(êµ°ì§‘ì˜ ìˆ˜)ê°€ ì–¼ë§ˆë‚˜ ì˜³ì€ì§€(ë™ì¼í•œ í´ë˜ìŠ¤ê°€ ë™ì¼ êµ°ì§‘ìœ¼ë¡œë§Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€)ê°€ ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¯¸ì„¸ì¡°ì •ì— ì˜í–¥ì„ ì£¼ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì„¤ê³„ë¨ - `MODE4`: `EXP8~EXP11`
  4) ë§ˆì§€ë§‰ìœ¼ë¡œ SBERTë¡œë¶€í„° ìƒì„±ëœ ì •ë³´ë¥¼ ëª¨ë¸ ì¸í’‹ìœ¼ë¡œ ì£¼ì…ì‹œí‚¤ëŠ” ê²ƒê³¼ initial hidden layerì— ì£¼ì…ì‹œí‚¤ëŠ” ê²ƒì˜ ì°¨ì´ë¥¼ PyTorchì™€ MXNet í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ë¹„êµí•¨ - `MODE4~MODE5`

<br>
<br>

The Mode 1, Mode 2, and Mode 3 experiments were designed to see how well a subject could classify 8 topics into the body and headlines of a news article.
  - `MODE 1`: It learns data containing only the body of news articles as input.
  - `MODE 2`: It learns data containing only news article titles as input.
  - `MODE 3`: It learns data containing both news article titles and body as input.

<br>

Mode 4 and Mode 5 are designed to create and experiment with a kind of distilled BERT model.
- This experiment is designed *to see if it can be fine-tuned a bit faster based on the clustering information generated by SBERT.*
- In this experiment, the information clustered by SBERT, a multilingual model, is used to fine-tune the KO-BERT model.
  - `MODE 4`: Cluster information generated from SBERT, a multilingual model, is used as model input, and this experiment used the mxnet framework in the same way as Modes 1 to 3.
  - `MODE 5`: Clustering information generated from SBERT was input to the initial layer using the PyTorch framework. The results of this experiment may be slightly different from Modes 1 to 4, so they are not described.
    - However, as in Mode 4, it was confirmed through several iterations that almost similar accuracy can be reached faster if you have cluster information generated from SBERT.


- You can see whole experiments result in [`experments/exp.md`](https://github.com/DSDanielPark/fine-tuned-korean-BERT-news-article-classifier/blob/main/experiments/exp.md) 
- In the case of EXP2 and EXP9, it was repeatedly performed to track and observe the learning rate, confirming similar learning patterns.

  | No | Condition | Best Test Accuracy | Epoch | 
  |:---:|:---|:---:|:---:|
  EXP1 | mode=1, batch_size=32 | 0.8842 | at Epoch 10
  EXP2 | mode=1, batch_size=32, epoch=20 without early stopping. | 0.8895 | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;at epoch 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  EXP3 | mode=1, batch_size=16 | 0.8800 | at epoch 4
  EXP4 | mode=1, batch_size=32 repeat of exp1 for check effect of randomness. | 0.8864 | at epoch 7
  EXP5 | mode=1, batch_size=32 | 0.8874 | at epoch 7 
  EXP6 | mode=2, batch_size=32 | 0.8269 | at epoch 7
  EXP7 | mode=3, batch_size=32 | 0.8864 | at epoch 6
  EXP8 | mode=4, batch_size=32, cluster_numb=8 | 0.8789 | at epoch 3
  EXP9 | mode=4, batch_size=32, cluster_numb=16 | 0.8895 | at epoch 5
  EXP10 | mode=4, batch_size=32, cluster_numb=32 | 0.8641 | at epoch 2
  EXP11 | mode=4, batch_size=32, cluster_numb=64 | 0.8885 | at epoch 7

<br>
<br>



# Evaluation
You can see evaluation metric of whole experiments in [`exp/exp_metric.md`](https://github.com/DSDanielPark/fine-tuned-korean-BERT-news-article-classifier/blob/main/experiments/exp_metric.md).

 <img src="./data/imgs/result.png" width="1000"><BR> *Fig 2. ê° ì‹¤í—˜ë³„ F1 score, Recall, Precision*

<br><br>
<br><br>ì°¸ê³ .
 F1 score is a performance metric for classification and is calculated as the harmonic mean of precision and recall.

<p align="center">
  <img src="./data/imgs/recall_precision.png" width="500"><br> 
  
</p>

*Fig 3. Reference figure to explain Recall, Prcision, Accuracy / Maleki, Farhad & Ovens, Katie & Najafian, Keyhan & Forghani, Behzad & Md, Caroline & Forghani, Reza. (2020). Overview of Machine Learning Part 1. Neuroimaging Clinics of North America. 30. e17-e32. 10.1016/j.nic.2020.08.007.*
 <br>

<br>

## Confusion Metrix and Heatmap in`EXP5` for each topic.
ì „ì²´ì ì¸ ê²°ê³¼ëŠ” [`exp/exp_metric.md`](https://github.com/DSDanielPark/fine-tuned-korean-BERT-news-article-classifier/blob/main/experiments/exp_metric.md)ì— ê³µê°œí•˜ì˜€ìœ¼ë©°, Evaluation ë„í‘œë¥¼ í†µí•´ ì¶©ë¶„íˆ ê²°ê³¼ì— ëŒ€í•œ ìœ ì¶”ê°€ ê°€ëŠ¥í•˜ë¯€ë¡œ ëª¨ë“  ì‹¤í—˜ì— ëŒ€í•œ Confusion Metrix ì œê³µì€ ìƒëµí•˜ë©°, ë‹¤ìŒ ì˜ˆì‹œë¥¼ í†µí•´ ê°ˆìŒí•¨.
<br>

1. Classification report

    | Topic |   Precision |   Recall |   F1-Score |    Support |
    |:--------------:|:------------:|:---------:|:-----------:|:-----------:|
    | Culture       |    0.833333 | `0.666667` |   0.740741 |  `15`        |
    | Economy       |    0.693878 | 0.871795 |   0.772727 | 117        |
    | Entertain     |    0.868132 | 0.963415 |   0.913295 |  82        |
    | International |    0.878049 | 0.870968 |   0.874494 | 124        |
    | IT            |    0.722222 | 0.722222 |   0.722222 |  18        |
    | Politics      |    0.965278 | 0.798851 |   0.874214 | 174        |
    | Society       |    0.874101 | 0.823729 |   0.848168 | 295        |
    | Sport         |    0.906977 | `1.000000`        |   0.95122  | `117`        |```

    |  avg  |   |   |    |     |
    |:--------------:|:------------:|:---------:|:-----------:|:-----------:|
    | accuracy      |    0.860934 | 0.860934 |   0.860934 |   0.860934 |
    | macro avg     |    0.842746 | 0.839706 |   0.837135 | 942        |
    | weighted avg  |    0.86909  | 0.860934 |   0.861426 | 942        |

<br>

2. Heatmap 

    <img src="./data/imgs/cm_exp5.png" width="500"> <br>
    *Fig 4. Heatmap of Confusion Metrix in `EXP5`*



<br>
<br>

# Embedding Token Visuallization<br>

  #### **1. SBERTë¡œë¶€í„° ìƒì„±ëœ í† í°ì˜ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ë¥¼ í†µí•´ K-means êµ°ì§‘í™”ë¥¼ ìˆ˜í–‰í•  ê²½ìš°, í´ëŸ¬ìŠ¤í„° ìˆ˜ ë³„ Ground Truth Category ë¶„í¬ ì‹œê°í™”**

  - íŒŒë€ì„ y-tick ë°´ë“œ ì‚¬ì´ê°€ ê° êµ°ì§‘ì„ ì˜ë¯¸
  - êµ°ì§‘ì•ˆì— ì¹´í…Œê³ ë¦¬ê°€ ì—†ì„ ê²½ìš° ì‹œê°í™”ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë°´ë“œê°€ ì¢ì„ìˆ˜ë¡ ì¡´ì¬í•˜ëŠ” Ground Truth Categoryì˜ ê³ ìœ ê°’ì˜ ìˆ˜ê°€ ì ìŒì„ ì˜ë¯¸
  - ì „ì²´ í…Œì´ë¸”ì€ [`csv`](https://github.com/DSDanielPark/fine-tuned-korean-BERT-news-article-classifier/blob/main/data/csv)í´ë” ì•ˆì—ì„œ í™•ì¸

    <img src="./data/imgs/cluster8.png" width="220" height='200'> <br>
    <img src="./data/imgs/cluster16.png" width="450" height='200'><br>
    <img src="./data/imgs/cluster32.png" width="900" height='200'><br>
    *Fig 5. í´ëŸ¬ìŠ¤í„°ë³„ Ground Truth Labelì˜ ë¶„í¬. ìœ„ì—ì„œë¶€í„° ì•„ë˜ ìˆœì„œë¡œ 8, 16, 32ê°œì˜ clusterë¡œ êµ°ì§‘í™”í•œ ê²°ê³¼, íŒŒë€ ë°´ë“œê°€ Clusterì˜ ê²½ê³„ë¥¼ ì˜ë¯¸í•˜ë©°, ë°´ë“œ ì‚¬ì´(í´ëŸ¬ìŠ¤í„°ë³„) Ground Truth Categoryì˜ ë¶„í¬ì •ë„ë¥¼ í‘œì‹œí•¨.*
    - SBERTë¡œ ì‚¬ì „ì— êµ°ì§‘ì— ëŒ€í•œ ì •ë³´ë¥¼ ì´ì‹í•˜ì—¬, KO-BERTì˜ fine-tunningì„ ì§„í–‰í•˜ì˜€ëŠ”ë°, EXP8ë¶€í„° EXP11ê¹Œì§€ 8ê°œ(`EXP8`)ë¶€í„° 64ê°œ(`EXP11`)ê¹Œì§€ êµ°ì§‘ì— ëŒ€í•œ ìˆ˜ë¥¼ ë³€ê²½í•˜ì—¬ ë´„. (64ê°œì˜ ê²½ìš° ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ì—†ìœ¼ë¯€ë¡œ í´ëŸ¬ìŠ¤í„°ë³„ Ground Truth Labelì˜ ë¶„í¬ ì‹œê°í™”ë¥¼ ìƒëµ)
    - ì‹¤í—˜ê²°ê³¼ëŠ” ê¸°ëŒ€ì™€ ë™ì¼í•˜ê²Œ 8ê°œì˜ êµ°ì§‘ì˜ ê²½ìš°, êµ°ì§‘ì´ ì¶©ë¶„íˆ ìœ ì‚¬í•œ ê¸°ì‚¬ë“¤ì„ ë¬¶ì–´ë‘ì§€ ëª»í•˜ì˜€ê³ (í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ê°€ ë„ˆë¬´ ì ìŒ),  32ê°œ ì´ìƒìœ¼ë¡œ ê³¼í•˜ê²Œ êµ°ì§‘í™”í•˜ì˜€ì„ ê²½ìš°, ë„ˆë¬´ ë§ì€ êµ°ì§‘ì˜ ìˆ˜ë¡œ ì¸í•´ ëª¨ë¸ í•™ìŠµì— ì¢‹ì§€ ì•Šì€ ì˜í–¥ì„ ë¯¸ì¹œ ê²ƒìœ¼ë¡œ ë³´ì„.
    - SBERTë¡œ ìƒì„±ëœ í† í°ì˜ ìœ ì‚¬ë„ë¥¼ ì„ë² ë”©í•´ì£¼ëŠ” ê²ƒì´ KO-BERT fine-tunningì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ëª©ì ì´ì˜€ìœ¼ë¯€ë¡œ, ì¶”ê°€ì ì¸ ìµœì  êµ°ì§‘ìˆ˜ë¥¼ ë„ì¶œì€ ë¶ˆìš”.
    - ë„ˆë¬´ ì ì€ êµ°ì§‘ì˜ ìˆ˜(<=the number of classes)ë‚˜ ë„ˆë¬´ ë§ì€ êµ°ì§‘ì˜ ìˆ˜(>=4 times the number of classes)ëŠ” ëª¨ë¸ í•™ìŠµì— ì¢‹ì€ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì„ í™•ì¸í•¨.
    - `EXP4`,`EXP11`ì˜ ê²°ê³¼ì™€ í•™ìŠµ ì–‘ìƒì´ ë¹„ìŠ·í•˜ë¯€ë¡œ ë„ˆë¬´ êµ°ì§‘ì˜ ìˆ˜ê°€ ë§ì€ EXP11ì—ì„œëŠ” KO-BERTì˜ ë¯¸ì„¸ì¡°ì •ì— SBERTë¡œë¶€í„° ìƒì„±ëœ Clusterì •ë³´ë¥¼ Representationìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ì¶”ì •í•¨. 
    - `MXNET` í”„ë ˆì„ì›Œí¬ì—ì„œëŠ” ì¸í’‹ ë°ì´í„°ì— êµ°ì§‘ì— ëŒ€í•œ ì •ë³´ë¥¼ ì‚½ì…í•˜ëŠ” ê²ƒìœ¼ë¡œ êµ¬í˜„í•˜ì˜€ê³ , `PyTorch` í”„ë ˆì„ì›Œí¬ì—ì„œëŠ” initial layerì— êµ°ì§‘ì— ëŒ€í•œ ì •ë³´ë¥¼ [CLS] í† í°ì— ì„ë² ë”©í•˜ì—¬ initial hidden layerì— ì •ë³´ë¥¼ ì‚½ì…í•˜ì˜€ìœ¼ë©°, í”„ë ˆì„ì›Œí¬ì™€ ìƒê´€ì—†ì´ ì¡°ê¸ˆ ë” ë¹ ë¥´ê²Œ, ë†’ì€ ì„±ëŠ¥ì— ë„ë‹¬í•  ìˆ˜ ìˆëŠ” ê²ƒì„ í™•ì¸í•¨.
      
  <br>
  <br>
  

  
  
  
  <br>

  #### **2. SBERTë¡œë¶€í„° ìƒì„±ëœ í† í° ì‹œê°í™”** <br>

  Multilingual BERT Embedding Spaceì— ëŒ€í•œ ì—°êµ¬[[10]](https://aclanthology.org/2022.findings-acl.103.pdf)ì™€ Recommender Systemsì—ì„œì˜ Embedding Spaceì— ëŒ€í•œ ì—°êµ¬[[11]](https://arxiv.org/abs/2105.08908)ì™€ ë¹„ìŠ·í•˜ê²Œ Tokenì´ ì¡°ê¸ˆ ë” Isotropicí•œ ë¶„í¬ë¥¼ ë³´ì´ëŠ” ê²ƒì´ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. ë³¸ ë ˆí¬ì§€í† ë¦¬ì—ì„œ í† í° ì‹œê°í™”ì— ì‚¬ìš©í•œ SBERT ì—­ì‹œ Multilingual BERTì´ë¯€ë¡œ ìƒê¸°[10]ì˜ ë…¼ë¬¸ê³¼ ë¹„ìŠ·í•œ í™˜ê²½ì—ì„œì˜ ì‹¤í—˜ì´ë¼ê³  ê°€ì •í•˜ëŠ” ê²ƒì— í° ë¬´ë¦¬ê°€ ì—†ì„ ê²ƒìœ¼ë¡œ íŒë‹¨í•˜ì˜€ë‹¤. <br> <br>ì œëª©ìœ¼ë¡œë¶€í„° ìƒì„±ëœ Embedding Spaceì—ì„œì˜ Tokenì˜ ë¶„í¬ê°€ ì¡°ê¸ˆ ë” isotropicí•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìœ¼ë©°, ì‹¤ì œë¡œ `EXP6`(ê¸°ì‚¬ì˜ ì œëª©ë§Œìœ¼ë¡œ í•™ìŠµ)ê³¼ `EXP7`(ê¸°ì‚¬ì˜ ë³¸ë¬¸ë§Œìœ¼ë¡œ í•™ìŠµ)ì˜ ê²°ê³¼ë¥¼ ë³´ë©´ Accuracy `0.8269`ì™€ `0.8864`ë¡œ ì•½ `0.595` ë” ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. 
  <br><br>
  ê²°ê³¼ì ìœ¼ë¡œ BERTì˜ fine-tunning íƒœìŠ¤í¬ ì´ì „ì— Low Dimension(2DIM or 3DIM) Spaceë¡œ ì‹œê°í™”í•˜ì—¬ ì¡°ê¸ˆ ë” isotropic token ë¶„í¬ë¥¼ ê°–ëŠ” ë°ì´í„°ì…‹ì´ë‚˜ ì „ì²˜ë¦¬ ë°©ë²•ì„ ì°¾ëŠ” ê²ƒì´ ì¡°ê¸ˆ ë” ì¢‹ì€ ëª¨ë¸ í•™ìŠµ ê²°ê³¼ë¥¼ ë³´ì¼ ìˆ˜ ìˆì„ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•œë‹¤. <br><br>
  2-1 ê¸°ì‚¬ì˜ ì œëª©ìœ¼ë¡œë¶€í„° ìƒì„±ëœ í† í° ì‹œê°í™”
  <img src="./data/imgs/title_token_vis.png" width="1000"><br>
  *Fig 6. Visualization of embedded tokens from SBERT for title of Korean articles*<br><br>
  2-2 ê¸°ì‚¬ ë³¸ë¬¸ìœ¼ë¡œë¶€í„° ìƒì„±ëœ í† í° ì‹œê°í™” <br>
  <img src="./data/imgs/body_token_vis.png" width="1000"> <br>
  *Fig 7. Visualization of embedded tokens from SBERT for body of Korean articles* <br><br>
  

  #### **3. KO-BERT fine-tunning ê³¼ì •ì—ì„œì˜ CLS Token ì‹œê°í™”**
  in the [HuggingFace BERT documentation](https://huggingface.co/docs/transformers/model_doc/bert#bertmodel), the returns of the BERT model are `(last_hidden_state, pooler_output, hidden_states[optional], attentions[optional])`
<br><BR>

# Discussion
- **If there is anything that needs to be corrected, productive criticism is always welcome.** <br>

  ### About total result
  - íƒœìŠ¤í¬ê°€ ê°„ë‹¨í•˜ì˜€ìœ¼ë¯€ë¡œ BERTì˜ fine-tuningë§Œìœ¼ë¡œ ë‹¨ê¸°ê°„ ì•ˆì— 0.8ì´ìƒì˜ ë¶„ë¥˜ê¸°ë¥¼ ìƒì„±í•¨.
  - ê¸°ì‚¬ì˜ ë³¸ë¬¸ì„ í¬í•¨í•  ê²½ìš°, `0.8641` ~ `0.8895`ì˜ í…ŒìŠ¤íŠ¸ ì…‹ Accuracyì˜ 8ê°œì˜ í† í”½ ë¶„ë¥˜ ëª¨ë¸ì„ ìƒì„±í•¨.
  - ê¸°ì‚¬ì˜ ì œëª©ë§Œì„ í¬í•¨í•  ê²½ìš°, `0.8269`ì˜ í…ŒìŠ¤íŠ¸ ì…‹ Accuracyë¥¼ í™•ì¸í•¨.
  - Sentence Embeddingí•œ ê²°ê³¼ë¥¼ BERT ëª¨ë¸ì˜ ì¸í’‹ìœ¼ë¡œ ì£¼ëŠ” ì‹¤í—˜ì—ì„œëŠ” Sentence Embeddingí•œ Clusterì˜ ìˆ˜ë³„ë¡œ ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆì—ˆìœ¼ë‚˜, ì•„ì£¼ ì ì€ ì •ë³´(1ìë¦¬ í˜¹ì€ 2ìë¦¬ì˜ ì •ìˆ˜)ë§Œìœ¼ë¡œ ì¡°ê¸ˆ ë” ë¹ ë¥´ê²Œ ìµœëŒ€ Accuracyì¸ `0.8895`ë¡œ ìˆ˜ë ´í•¨ì„ í™•ì¸í•¨.
  - ì „ë°˜ì ìœ¼ë¡œ ì¶©ë¶„í•œ ì‹¤í—˜ì´ ìˆ˜í–‰ë˜ì§„ ì•Šì•˜ì§€ë§Œ, ëª‡ ê°€ì§€ ì¡°ê±´ì— ëŒ€í•´ ë°˜ë³µì ì¸ í•™ìŠµì„ ì§„í–‰í•´ ë³¸ ê²°ê³¼, í˜„ì¬ ì‹¤í—˜ì¡°ê±´ì—ì„œì˜ ìµœê³  ì„±ëŠ¥ì€ `0.8895` ì •ë„ì˜€ê³ , ì‚¬ì „ì— Clusterì— ëŒ€í•œ ì •ë³´ë¥¼ ì¤„ ê²½ìš°, ìµœê³  ì„±ëŠ¥ì— ë” ë¹ ë¥´ê²Œ ë„ë‹¬í•¨ì„ í™•ì¸í•¨.
  - ê¸°ëŒ€ì™€ ë™ì¼í•˜ê²Œ, ì œëŒ€ë¡œ ëœ ì•½ê°„ì˜ ì´ˆê¸° í´ëŸ¬ìŠ¤í„° ì •ë³´ë§Œ ìˆì–´ë„ BERT ëª¨ë¸ì´ ì¡°ê¸ˆ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆê³ , BERT ëª¨ë¸ì˜ ê²½ìš° Early Stop ì¡°ê±´ì„ ì¡°ê¸ˆ ê´€ëŒ€í•˜ê²Œ ì£¼ëŠ” ê²ƒì´ ì¢‹ìŒì„ í™•ì¸í•¨.
  - ìœ„ ì½”ë“œ ì‘ì—…ì´ë‚˜ ì‹¤í—˜ì´ ë§¤ìš° ë‹¨ê¸°ê°„(ì •í™•ë„ 0.88ëŒ€ì˜ base-line ë„ì¶œê¹Œì§€ ëŒ€ëµ 3ì¼)ì— ì´ë¤„ì¡Œê³ , KO-BERT fine-tuning ì„±ëŠ¥ì„ í™•ì¸í•˜ê¸° ìœ„í•œ ê²ƒì— ì´ˆì ì„ ë§ì¶° êµ¬ì„±ë˜ì—ˆìŒ.
  - ë³¸ íƒœìŠ¤í¬ëŠ” 1) ë°ì´í„°ì— ë¹„í•´ íƒœìŠ¤í¬ ë‚œì´ë„ê°€ ë§¤ìš° ë‚®ê³ , 2) ê³ ë„ì˜ ì •í™•ë„ë¥¼ ìš”êµ¬í•˜ì§€ ì•Šìœ¼ë©°, 3) ëª‡ê°€ì§€ íŠ¸ë¦­ ë° ë£°ë¡œ ë” ë¹ ë¥´ê²Œ ë¶„ë¥˜ê°€ ê°€ëŠ¥í•˜ë¯€ë¡œ BERTë¥¼ í•´ë‹¹ íƒœìŠ¤í¬ì— ì ìš©í•˜ê¸° ìœ„í•œ ì‹¤í—˜ì´ë¼ê¸° ë³´ë‹¤ëŠ” ë‹¤ë¥¸ ì–´ë ¤ìš´ íƒœìŠ¤í¬ì— BERT ëª¨ë¸ì˜ ì ìš©í•  ë•Œ ìœ ìš©í•œ ëª‡ê°€ì§€ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ì„œ êµ¬ì„±ë˜ì—ˆìŒ. 
  - ì¶”ê°€ì ìœ¼ë¡œ GPT-3ë¥¼ ìœ„í•œ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ë³´í•˜ì˜€ìœ¼ë¯€ë¡œ, ë™ì¼í•œ íƒœìŠ¤í¬ì™€ ì‹¤í—˜ë“±ì„ í†µí•´ GPT-3ì™€ BERT ëª¨ë¸ì˜ ì ìš© ë¹„êµë¥¼ ì¶”ê°€í•˜ê³ ì í•¨.
  <br>
  <br>  

  ### Further experiments
  - `(Qualitative check)` ì „ì²˜ë¦¬ ì—†ì´ ë°ì´í„° ì…‹ì˜ ì›ë³¸ë§Œìœ¼ë¡œë„ ìš”êµ¬ì‚¬í•­ì´ì—ˆë˜ 70% ì´ìƒì˜ ì •í™•ë„ë¥¼ ë³´ì˜€ìœ¼ë¯€ë¡œ, ì¶”ê°€ì ì¸ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•˜ì§€ ì•Šì•˜ì§€ë§Œ, ì–´ë–¤ í† í°ì´ ëª¨ë¸ ì¸í¼ëŸ°ìŠ¤ì— ì˜í–¥ì„ ì£¼ëŠ”ì§€ í™•ì¸í•˜ê³ , ëª¨ë¸ì´ ì¸í’‹ ê¸°ì‚¬ ë°ì´í„°ì—ì„œ ì´ìƒí•œ í† í°ì„ Representationìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€ëŠ” ì•Šì€ì§€ ì²´í¬í•´ ë³¼ í•„ìš”ê°€ ìˆìŒ.
  - `(eXplainable AI)`ì¶”ê°€ì ìœ¼ë¡œ í† í°ì˜ ì˜í–¥ë„ë¥¼ ì‹œê°í™”í•´ì„œ SBERTë¡œ ì¤€ Clusterê°€ ì–´ëŠ ì •ë„ì˜ ì˜í–¥ì´ ìˆì—ˆëŠ”ì§€ í™•ì¸í•  ê³„íš.
  - `(Understanding the BERT model)` ì¶”ê°€ì ìœ¼ë¡œ ë°°ì¹˜ ì‚¬ì´ì¦ˆì— ëŒ€í•œ ì°¨ì´, BERT ëª¨ë¸ì´ ë³¼ ìˆ˜ ìˆëŠ” max_lengthì˜ ì°¨ì´ë¥¼ ê´€ì°°í•˜ë©´ í–¥í›„ fine-tuningí•  ë•Œ í° ë„ì›€ì´ ë  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ë³´ì´ë©°, GPT-3ì— ëŒ€í•œ ê²ƒë“¤ì€ í–¥í›„ ë¦¬ì†ŒìŠ¤ê°€ í™•ë³´ë˜ëŠ”ëŒ€ë¡œ ë‹¤ì‹œ ë¹„ìŠ·í•˜ê²Œ ì‰¬ìš´ íƒœìŠ¤í¬ë¡œ ì²´í¬í•´ ë³¼ ê³„íš.
  - `(Exploring the Sentence Embedding Cluster)` Sentence Embeddingì˜ ê²½ìš°, Cluster ìˆ˜ì— ë”°ë¼ì„œ ì–´ëŠ ì •ë„ ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ë³´ì¸ë‹¤ê³  íŒë‹¨ë˜ë©°, ëª‡ê°€ì§€ ì¶”ê°€ ì‹¤í—˜ì„ ë” ìˆ˜í–‰í•´ë³¼ ìˆ˜ ìˆìŒ.
  - `(Hypothesis Testing)`ê°€ì •í•˜ì˜€ë˜ëŒ€ë¡œ ìœ ì‚¬ë„ í´ëŸ¬ìŠ¤í„°ë¥¼ ì´ˆê¸°ê°’ìœ¼ë¡œ ê°–ê³  ìˆì„ê²½ìš°, ë” ë¹ ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìˆìŒì„ í™•ì¸í•˜ì˜€ìœ¼ë©°, ê²°ë¡ ì ìœ¼ë¡œ Sentence Embeddingìœ¼ë¡œë¶€í„° ì–»ì€ ì•„ì£¼ ì‘ì€ ì–‘ì˜ ì„íŒ©íŠ¸ ìˆëŠ” í´ëŸ¬ìŠ¤í„° ì •ë³´(high quality and low volume of information)ë§Œìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ ì†ë„ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ í™•ì¸í•¨.
      - ì´ëŠ” multilingual-BERTì¸ SBERTì˜ í† í° ì„ë² ë”© ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ì •ë³´ë¥¼ ìƒì„±í•˜ê³ , KO-BERTê°€ fine-tuning ê³¼ì •ì—ì„œ SBERTì˜ ì•„ì›ƒí’‹ì„ ì°¸ì¡°í•˜ë¯€ë¡œ, ì¼ì¢…ì˜ ì§€ì‹ ì¦ë¥˜ì˜ ì„±ê²©ì„ ëˆë‹¤ê³  ë³¼ ìˆ˜ ìˆìœ¼ë©°, í•™ìŠµ ì†ë„ë©´ì—ì„œ ê²°ê³¼ ì°¨ì´ê°€ ìˆìŒì„ í™•ì¸í•¨.
      - ì´ëŸ¬í•œ ì¦ë¥˜ëŠ” ì ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” íƒœìŠ¤í¬ì—ì„œ í° ì‹¤ìµì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë‚˜, í´ëŸ¬ìŠ¤í„° ì •ë³´ë¥¼ ì¸í’‹ìœ¼ë¡œ ë„£ëŠ” ê²ƒì´ ë” í¬ê³  ë¬´ê±°ìš´ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ í•™ìŠµì‹œí‚¤ë„ë¡ ê¸°ì—¬í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤Œ.
      - ë‹¤ë¥¸ ëª¨ë¸ë¡œë¶€í„° ì „ë‹¬ë˜ëŠ” ë ˆì´ë¸” ê°’ì€ ëª¨ë¸ì´ ì°¸ê³ í•˜ê¸°ì— ì¶©ë¶„íˆ ì˜³ì€ ì •ë³´ë§Œìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´ì•¼í•œë‹¤ëŠ” ê²ƒì„ í™•ì¸í•¨.
        - `EXP8` ~ `EXP11`ì´ ì´ëŸ¬í•œ í´ëŸ¬ìŠ¤í„° ì •ë³´ì˜ ì§ˆì— ë”°ë¥¸ í•™ìŠµì†ë„ ë° í•™ìŠµ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ êµ¬ì„±ë˜ì—ˆìœ¼ë©°, 8ê°œì˜ í´ë˜ìŠ¤ ë¶„ë¥˜ê¸° ìƒì„±ì‹œì— 8ê°œì˜ í´ëŸ¬ìŠ¤í„° ì •ë³´ë³´ë‹¤ëŠ” 16ê°œì •ë„ ì¶©ë¶„íˆ ì˜³ì€ í´ëŸ¬ìŠ¤í„° ì •ë³´ë§Œì„ ê°–ê³  ìˆë„ë¡ êµ¬ì„±í•˜ëŠ” ê²ƒì´ ì¢‹ìŒì„ í™•ì¸í•¨.ë¡œ í† í°ì˜ ì˜í–¥ë„ë¥¼ ì‹œê°í™”í•´ì„œ SBERTë¡œ ì¤€ Clusterê°€ ì–´ëŠ ì •ë„ì˜ ì˜í–¥ì´ ìˆì—ˆëŠ”ì§€ í™•ì¸í•  ê³„íš.

<br>

### `MODE1 & MODE3` 
  - If the sentence length is sufficiently secured, the title of the article is just information that is a repetition of important keywords in the article. Thus it showed a similar accuracy whether the title was added or not. The accuracy after the first epoch can be ignored as the result of random initialization.
  - The title of the article implies the connotation of very well-refined information. Therefore, it is possible to consider giving a little more weight to the title. However, considering the complexity, it is not worth that much.


### `MODE2` 
  - The BERT model has strengths in semantic inference through a slightly wider context and self-emphasis in the context than existing natural language processing models. Therefore, it shows that it is very difficult to classify the topic with only the use of article titles(limited length).
  - Nevertheless, it was surprising that the title of the article alone could be inferred with 82% accuracy just by fine-tuning the BERT model.
  - If I could infer using GPT-3, the result would be much better in the same condition.


### `MODE4 & MODE5` 
Mode 4 and Mode 5 are designed to create and experiment with a kind of distilled BERT model.
  - This experiment is designed *to see if it can be fine-tuned a bit faster based on the clustering information generated by SBERT.*
  - Basic Architecture of BERT models is Next token prediction, hence it is performed to compare the difference between giving information about BERT-based clustering information to the initial hidden layer and just putting information about clustering in the sentence.
  - K-means clustering is performed through the euclidean distance of the latent vector generated from the multilingual-BERT model [5].
  - Since two pretrained BERT model weights were used in the entire pipeline, only information was distilled and embedded when it was determined to be significant to prevent model confusion.
<br>
<br>
  
### Distil BERTì— ê´€ë ¨ëœ ì‹¤í—˜ì¸ `EXP2 & EXP9`ì— ëŒ€í•œ ê³ ì°°   
  - ìœ„ `Embedding Token Visuallization` - 1ì˜ ì—°ì¥ì„ ìœ¼ë¡œ EXP2ì™€ EXP9ì˜ ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ê³  ì‹œê°í™”í•¨.
  
  <p align="center">
    <img src="./data/imgs/Precision2and9.png" width="300"> <BR> 
  </p>

  *Fig . ì‹¤í—˜2ì™€ ì‹¤í—˜3ì˜ Precision ë¹„êµ*


  |   exp | metric    |   culture |    economy |   entertain |   international |        it |   politics |    society |      sport |   accuracy |   macro avg |   weighted avg |
  |------:|:----------|----------:|-----------:|------------:|----------------:|----------:|-----------:|-----------:|-----------:|-----------:|------------:|---------------:|
  |     2 | f1-score  |  0.714286 |   0.801802 |    0.911243 |        0.878049 |  0.769231 |   0.8739   |   0.874791 |   0.975000    |   0.877919 |    0.849788 |       0.877036 |
  |     9 | f1-score  |  0.774194 |   0.796610  |    0.898204 |        0.904000    |  0.829268 |   0.912387 |   0.862543 |   0.951220  |   0.881104 |    0.866053 |       0.881093 |
  |     2 | precision |  0.769231 |   0.847619 |    0.885057 |        0.885246 |  0.714286 |   0.892216 |   0.861842 |   0.951220  |   0.877919 |    0.85084  |       0.877594 |
  |     9 | precision |  0.750000     |   0.789916 |    0.882353 |        0.896825 |  0.73913  |   0.961783 |   0.874564 |   0.906977 |   0.881104 |    0.850194 |       0.883224 |
  |     2 | recall    |  0.666667 |   0.760684 |    0.939024 |        0.870968 |  0.833333 |   0.856322 |   0.888136 |   1        |   0.877919 |    0.851892 |       0.877919 |
  |     9 | recall    |  0.800000      |   0.803419 |    0.914634 |        0.911290  |  0.944444 |   0.867816 |   0.850847 |   1        |   0.881104 |    0.886556 |       0.881104 |
  |   nan | support   | 15        | 117        |   82        |      124        | 18        | 174        | 295        | 117        |   0.877919 |  942        |     942        |

<br>
<br>
  
# Install Issues [Optional]
- Although my time was dead, I want to save your time.
- In `Google Colab`, some resources can handle version conflict, but the others failed to handle this issue. So you need to try 3-4 times while changing resource type(GPU or CPU).<br>
- All issues are caused by legacy `numpy` versions in each package. 
- Do not try to use mxnet acceleration because mxnet is very incomplete. The issues about mxnet acceleration in some cases have not been closed for a year.
- `**CLONE REPOSITORY AND USE THE MODULES AS IT IS - HIGHLY RECOMMENDED**`

<br>
<br>


#### * About `mxnet` and `PyTorch` Framework
- ì»¤ë®¤ë‹ˆí‹°ì™€ í¸ì˜ì„±, ì´ìŠˆ ì—†ëŠ” ì•ˆì •ì„± ë“± ê±°ì˜ ëª¨ë“  ë©´ì—ì„œ PyTorchê°€ ì´ì ì´ ìˆìŒ.

#### * About `mxnet` install (numpy version conflict)
- If you have a problem in the process of installing `mxnet`, you need to use `python=<3.7.0`.
- Any other options can not solve the numpy version conflict problem while `pip install mxnet`. 
- Except using python=3.7.xx, *EVERYTHING IS USELESS.*
  ```
  error: subprocess-exited-with-error Ã— 

  python setup.py bdist_wheel did not run successfully. â”‚ exit code: 1 
  â•°â”€> [890 lines of output] Running from numpy source directory. 

  C:\Users\user\AppData\Local\Temp\pip-install-8vepm8z2\numpy_34577a7b4e0f4f25959ef5aa089c5687\numpy\distutils\misc_util.py:476: SyntaxWarning: "is" with a literal. Did you mean "=="? return is_string(s) and ('*' in s or '?' is s) blas_opt_info: blas_mkl_info: No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils
  ```
- You may try to install `mxnet` before installing `gluonnlp`.
<br>



#### * About KO-BERT version conflict
- As a result of conflict from above note(About `mxnet` install), KO-BERT still has version conflict.
  ```
  INFO: pip is looking at multiple versions of boto3 to determine which version is compatible with other requirements. This could take a while.
  ...
  To fix this you could try to:
  1. loosen the range of package versions you've specified
  2. remove package versions to allow pip attempt to solve the dependency conflict
  ```
  **Solution of this version conflict**
  - If you DO NOT use mxnet with KO-BERT, then remove mxnet in kO-bert `requirements.txt` or adjust(loosen) version as your environments.
  - If you DO use mxnet with KO-BERT, then just clone the repository and import the module as it is.

<br>

#### * About random initialization in mxnet


  ```
  RuntimeError: Parameter 'bertclassifier2_dense0_weight' has not been initialized. Note that you should initialize parameters and create Trainer with Block.collect_params() instead of Block.params because the later does not include Parameters of nested child Blocks.
  ```

- Even when random initialization is not normally performed due to variable management problems, you can observe indicators that seem to be successfully learned in the trainset. 
- However, it was observed that the performance of the trainset continued to drop because it could not be directed to the candidate space of other local minima in the loss space. 
- Therefore, unlike other models, in the case of the Bert model, it is recommended to perform indicator checks in the trainset every iters.
- When using the Apache mxnet framework, carefully organize which layers to lock and which layers to update and in what order. Even when refactoring, initialize and update layers carefully. => to save your time.

<br>

#### * About Tokenizer Error

  ```
  The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
  The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. 
  The class this function is called from is 'KoBERTTokenizer'.
  ```

- First of all, check that num_classes are the same. And make sure classes of torch and mxnet framework are not used interchangeably. Finally, check if there is a conflict or mixed use in the wrapped class as written below. (Ignore the following if it has been resolved.)
- As mentioned above, If it is impossible to download the pretrained KOBERT weights from the aws server, you can download the wrapped weights to the hugging face interface through XLNetTokenizer. [ [3](https://github.com/SKTBrain/KoBERT/tree/master/kobert_hf) ]

<br>

#### * About Korean NLP Models
- Almost all Korean NLP Models have not been updated often. Thus you should try to install lower versions of packages.
- recommendation: `python<=3.7.0` <br>
**Some Tips**
  - IndexError: Target 2 is out of bounds. => num_classes error (please check)
  - broken pipe error => num_workers error, if you use CPU => check your num of threads, else remove args numworkers.
  - If you can not download torch KO-BERT weight with urlib3 or boto3 library error message include 'PROTOCOL_TLS' issue, This is an error related to Amazon aws server download. Thus, => use huggingface interface https://github.com/SKTBrain/KoBERT/tree/master/kobert_hf
  - If you have other questions, please send me an e-mail. *All is Well!! Happy Coding!!*
  - ì´ìŠˆëŠ” issueë¥¼ ìƒì„±í•´ì£¼ì‹œê±°ë‚˜, ë©”ì¼ë¡œ ë¬¸ì˜ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.

<br>

# References <Br>
[1] GPT fine-tune https://www.philschmid.de/getting-started-setfit <br>
[2] KO-GPT https://github.com/kakaobrain/kogpt <br>
[3] KO-BERT https://sktelecom.github.io/project/kobert <br>
`download` of kobert-base-v1 https://huggingface.co/gogamza/kobart-base-v1 <br>
[4] Sentence-embedding https://github.com/UKPLab/sentence-transformers <br>
[5] Multilingual SBERT https://www.sbert.net/examples/training/multilingual/README.html <br>
[6] NLP gluon BERT documentation https://nlp.gluon.ai/model_zoo/bert/index.html
<br>
ì°¸ê³ í•˜ì§€ ì•Šì•˜ì§€ë§Œ, BERT model few shot learning ê´€ë ¨ëœ ë…¼ë¬¸ì„ ì°¾ë‹¤ mode4ì™€ ë¹„ìŠ·í•œ ì»¨ì…‰ì˜ ë…¼ë¬¸ì„ í™•ì¸í•¨ <br>
[7]
  [Evaluation of Pretrained BERT Model by Using Sentence Clustering](https://aclanthology.org/2020.paclic-1.32) (Shibayama et al., PACLIC 2020)
<Br>
[8] T-SNE ê´€ë ¨ https://www.nature.com/articles/s41467-019-13056-x <br>
[9] DistilBERT https://arxiv.org/abs/1910.01108 <br>
[10] An Isotropy Analysis in the Multilingual BERT Embedding Space https://aclanthology.org/2022.findings-acl.103.pdf
<br>
[11] Where are we in embedding spaces? https://arxiv.org/abs/2105.08908 <br>
[12] Visuallization of BERT https://github.com/jessevig/bertviz.git <br>
 - shap https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/text.html
 - captum https://captum.ai/ <Br>
 
[13] Hugging face model weight upload and load
- https://huggingface.co/transformers/v1.0.0/model_doc/overview.html#loading-google-ai-or-openai-pre-trained-weights-or-pytorch-dump
- https://huggingface.co/docs/transformers/main_classes/model
- https://huggingface.co/docs/huggingface_hub/how-to-downstream
 <br><br>

#### Daily Commit Summary <br>
|Date|Description|
|:---:|:---|
|23.01.16|* ìì›ì²´í¬: GPT inference ìµœì†Œ VRAM ìš”êµ¬ ìš©ëŸ‰(32GB) ë¶€ì¡±ìœ¼ë¡œ GPT ì‚¬ìš©ë¶ˆê°€, KO-BERT ì‚¬ìš©  <br> - í™˜ê²½ì…‹ì—… ì™„ë£Œ: ë¡œì»¬ ìì›ê³¼ Colab ë³‘í–‰ <br> - ê°„ë‹¨í•œ Problem Definition and EDA, Data Analysis, BERT Embbeding Visuallization&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|
|23.01.17|- ë² ì´ìŠ¤ ì½”ë“œ ì‘ì„± <br> - ê¸°ê°„ ë‚´ ìˆ˜í–‰ ê°€ëŠ¥í•œ ì‹¤í—˜ ë¦¬ìŠ¤íŠ¸ ì‘ì„±|
|23.01.18|- ì‹¤í—˜ê²°ê³¼ ì •ë¦¬ <br>- íŒŒì´í”„ë¼ì¸ í™•ì •|
|23.01.19~|- ìµœì¢… ì œì¶œ ë° ë¦¬íŒ©í† ë§ í›„ Repository ì •ë¦¬ <br>- Documentation <br> - Recommendation projectì— ì„œë¸Œ ëª¨ë“ˆë¡œ ì‚¬ìš©(t-sneë‘ embeddingë¶€ë¶„ í¬í•¨)|
|23.02.05~|- Kaggle í´ë¦­, ì¥ë°”êµ¬ë‹ˆ, êµ¬ë§¤ í•­ëª© ì˜ˆì¸¡ ëª¨ë¸ë§í•˜ë©° Colab Proë¥¼ ê²°ì œí–ˆìœ¼ë¯€ë¡œ 2ì›”ì¤‘ì— GPTë¥¼ í†µí•œ í•™ìŠµ ì‹¤í—˜ ì—…ë°ì´íŠ¸ ì˜ˆì • <br> - XAI, FrontEndë„ ì¶”ê°€ì ìœ¼ë¡œ ìˆ˜í–‰ ì˜ˆì •|

<br>

#### Further Reading...
[1] PyTorch CRF https://github.com/kmkurn/pytorch-crf
